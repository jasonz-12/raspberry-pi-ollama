{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century. Here's a simplified explanation:\n",
      "\n",
      "When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules are much smaller than the wavelength of light, which means they scatter the light in all directions.\n",
      "\n",
      "The shorter wavelengths of light, like blue and violet, are scattered more than the longer wavelengths, like red and orange. This is because the smaller molecules have a greater tendency to absorb and scatter the shorter wavelengths.\n",
      "\n",
      "As a result, the blue light is dispersed throughout the atmosphere, giving the sky its blue color. The exact shade of blue can vary depending on atmospheric conditions, such as pollution, dust, and water vapor, but overall, the scattering effect dominates.\n",
      "\n",
      "It's worth noting that during sunrise and sunset, the sky often appears red or orange due to a phenomenon called Mie scattering, which occurs when shorter wavelengths of light are scattered by larger particles in the atmosphere. However, this is not why the entire sky appears blue during these times.\n",
      "\n",
      "I hope that helps you understand why the sky is blue!\n",
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century. Here's a simplified explanation:\n",
      "\n",
      "When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules are much smaller than the wavelength of light, which means they scatter the light in all directions.\n",
      "\n",
      "The shorter wavelengths of light, like blue and violet, are scattered more than the longer wavelengths, like red and orange. This is because the smaller molecules have a greater tendency to absorb and scatter the shorter wavelengths.\n",
      "\n",
      "As a result, the blue light is dispersed throughout the atmosphere, giving the sky its blue color. The exact shade of blue can vary depending on atmospheric conditions, such as pollution, dust, and water vapor, but overall, the scattering effect dominates.\n",
      "\n",
      "It's worth noting that during sunrise and sunset, the sky often appears red or orange due to a phenomenon called Mie scattering, which occurs when shorter wavelengths of light are scattered by larger particles in the atmosphere. However, this is not why the entire sky appears blue during these times.\n",
      "\n",
      "I hope that helps you understand why the sky is blue!\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2:1b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  How are you?\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Load the model\n",
    "model = whisper.load_model(\"tiny\")  # Options: tiny, base, small, medium, large\n",
    "\n",
    "# Transcribe an audio file\n",
    "result = model.transcribe(\"./audio/speech.mp3\")  # Replace with your audio file path\n",
    "\n",
    "# Print the result\n",
    "print(\"Transcription:\", result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TTS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get device\u001b[39;00m\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'TTS'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# List available üê∏TTS models\n",
    "print(TTS().list_models())\n",
    "\n",
    "# Init TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "# Run TTS\n",
    "# ‚ùó Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n",
    "# Text to speech list of amplitude values as output\n",
    "wav = tts.tts(text=\"Hello world!\", speaker_wav=\"audio.wav\", language=\"en\")\n",
    "# # Text to speech to a file\n",
    "# tts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "text:   4%|‚ñç         | 16/384(max) [00:00, 40.36it/s]\n",
      "code:   6%|‚ñå         | 126/2048(max) [00:01, 76.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import ChatTTS\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "chat = ChatTTS.Chat()\n",
    "chat.load(compile=False) # Set to True for better performance\n",
    "\n",
    "texts = [\"Hope you are doing great in this fine morning.\"]\n",
    "\n",
    "wavs = chat.infer(texts)\n",
    "\n",
    "for i in range(len(wavs)):\n",
    "    \"\"\"\n",
    "    In some versions of torchaudio, the first line works but in other versions, so does the second line.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torchaudio.save(f\"basic_output{i}.wav\", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)\n",
    "    except:\n",
    "        torchaudio.save(f\"basic_output{i}.wav\", torch.from_numpy(wavs[i]), 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "class CoquiTTSWrapper:\n",
    "    def __init__(self, model_name=\"tts_models/en/ljspeech/vits\", device=\"cpu\"):\n",
    "        \"\"\"Initialize TTS with specified model\"\"\"\n",
    "        try:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() and device == \"cuda\" else \"cpu\"\n",
    "            self.tts = TTS(model_name=model_name, progress_bar=True).to(self.device)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to initialize TTS: {str(e)}\")\n",
    "        \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        return TTS().list_models()\n",
    "    \n",
    "    def synthesize(self, text, output_path=None, sample_rate=22050):\n",
    "        \"\"\"Generate speech from text\"\"\"\n",
    "        try:\n",
    "            wav = self.tts.tts(text=text)\n",
    "            \n",
    "            if output_path:\n",
    "                sf.write(output_path, wav, sample_rate)\n",
    "            \n",
    "            return wav, sample_rate\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Speech synthesis failed: {str(e)}\")\n",
    "    \n",
    "    def play_audio(self, wav, sample_rate=22050):\n",
    "        \"\"\"Play audio using sounddevice\"\"\"\n",
    "        try:\n",
    "            sd.play(wav, sample_rate)\n",
    "            sd.wait()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Audio playback failed: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Install required packages if not already installed:\n",
    "    # pip install TTS torch sounddevice soundfile numpy\n",
    "    \n",
    "    try:\n",
    "        # Initialize TTS\n",
    "        tts = CoquiTTSWrapper()\n",
    "        \n",
    "        # Generate and play speech\n",
    "        text = \"Hope you're doing great in this fine morning!\"\n",
    "        wav, sr = tts.synthesize(text, \"output.wav\")\n",
    "        tts.play_audio(wav, sr)\n",
    "        \n",
    "        # List available models\n",
    "        print(\"\\nAvailable models:\")\n",
    "        for model in tts.list_models():\n",
    "            if \"en\" in model:  # Filter for English models\n",
    "                print(f\"- {model}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import time\n",
    "\n",
    "def generate_tone(frequency, duration, amplitude=0.2, sample_rate=44100):\n",
    "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
    "    return amplitude * np.sin(2 * np.pi * frequency * t)\n",
    "\n",
    "# Generate two tones for \"do-dom\"\n",
    "tone1 = generate_tone(440, 0.3)  # First tone: 440 Hz (A4)\n",
    "tone2 = generate_tone(500, 0.3)  # Second tone: 392 Hz (G4)\n",
    "\n",
    "# Concatenate with a short silence in between\n",
    "silence = np.zeros(int(0.1 * 44100))  # 50 ms silence\n",
    "siri_sound = np.concatenate((tone1, silence, tone2))\n",
    "\n",
    "# Play the sound\n",
    "sd.play(siri_sound, samplerate=44100)\n",
    "sd.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
